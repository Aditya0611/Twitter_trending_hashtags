# Automated Twitter Trends Scraper for Supabase

This project contains a Python script that automatically scrapes the latest trending topics for India from [trends24.in](https://trends24.in/india/), analyzes each topic for sentiment and engagement, and stores the processed data in a Supabase database.

The entire process is automated to run every 3 hours using a GitHub Actions workflow.

## Features

- **Web Scraping**: Fetches top 9 trending hashtags for India.
- **Data Processing**: Parses tweet counts and generates relevant metadata.
- **Sentiment Analysis**: Calculates a sentiment score (Positive, Negative, Neutral) for each hashtag using `TextBlob`.
- **Engagement Scoring**: Calculates a custom engagement score (1-10) based on tweet volume and keywords.
- **Database Integration**: Clears old data and inserts the fresh trends into a Supabase table.
- **Full Automation**: A GitHub Actions workflow runs the script on a schedule (`0 */3 * * *`).

## Tech Stack

- **Language**: Python 3
- **Libraries**:
  - `requests` & `BeautifulSoup4` for web scraping.
  - `supabase` for database interaction.
  - `textblob` for sentiment analysis.
  - `python-dotenv` for managing environment variables locally.
- **Database**: Supabase (PostgreSQL)
- **Automation**: GitHub Actions

---

## Setup and Installation

### 1. Prerequisites
- Python 3.8 or higher
- Git
- A Supabase account

### 2. Clone the Repository
```bash
git clone https://github.com/your-username/your-repository-name.git
cd your-repository-name
```

### 3. Set Up a Python Virtual Environment
It's highly recommended to use a virtual environment.
```bash
# Windows
python -m venv .venv
.\.venv\Scripts\activate

# macOS / Linux
python3 -m venv .venv
source .venv/bin/activate
```

### 4. Install Dependencies
```bash
pip install -r requirements.txt
```

### 5. Set Up Supabase
1.  Create a new project in your [Supabase dashboard](https://app.supabase.io).
2.  Go to the **SQL Editor** and run the following query to create the `twitter` table:
    ```sql
    CREATE TABLE public.twitter (
      id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
      created_at TIMESTAMPTZ DEFAULT NOW(),
      platform TEXT,
      topic_hashtag TEXT,
      engagement_score REAL,
      sentiment_polarity REAL,
      sentiment_label TEXT,
      posts INTEGER,
      views INTEGER,
      metadata JSONB,
      version_id UUID
    );
    ```
3.  Go to **Project Settings** > **API**. Find your **Project URL** and your `service_role` **Secret Key**. You will need these next.

### 6. Configure Environment Variables (for Local Testing)
Create a file named `.env` in the root of the project directory. **This file should not be committed to Git.**
```.env
SUPABASE_URL="YOUR_SUPABASE_PROJECT_URL"
SUPABASE_KEY="YOUR_SUPABASE_SERVICE_ROLE_KEY"
```

---

## Running the Script Locally

To test the script on your local machine, simply run the Python file.
```bash
python twitter_scraper.py
```
Check your Supabase table to see if the data was inserted.

---

## Automation with GitHub Actions

This repository is configured to run the scraper automatically.

1.  **Workflow File**: The automation logic is defined in `.github/workflows/twitter_scraper.yml`.
2.  **Schedule**: The script is scheduled to run **every 3 hours**.
3.  **Secrets**: For the automation to work, you **must** add your Supabase credentials to your GitHub repository's secrets:
    - Go to your repository on GitHub > `Settings` > `Secrets and variables` > `Actions`.
    - Click `New repository secret`.
    - Create a secret named `SUPABASE_URL` and paste your Supabase URL.
    - Create another secret named `SUPABASE_KEY` and paste your Supabase `service_role` key.

Once the secrets are set, the workflow will run automatically on the next scheduled time, or you can trigger it manually from the "Actions" tab in your GitHub repository.

## File Structure

```
.
├── .github/
│   └── workflows/
│       └── twitter_scraper.yml   # The GitHub Actions workflow file
├── .venv/                          # Virtual environment folder (ignored)
├── twitter_scraper.py              # The main Python script
├── requirements.txt                # List of Python dependencies
├── .env                            # Local environment variables (ignored)
└── README.md                       # This file
```